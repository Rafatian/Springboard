{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() \n",
    "import dask.dataframe as dd\n",
    "import multiprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data using pandas library and creating an hdf5 file using vaex \n",
    "\n",
    "cols = ['Target', 'ID', 'Date', 'Query', 'User', 'Text']\n",
    "raw_tweets = pd.read_csv('sentiment_140/training.1600000.processed.noemoticon.csv', encoding='latin-1', names = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most important thing is the text and the sentiment of it so we will get rid of the rest of the columns\n",
    "\n",
    "raw_tweets = raw_tweets[['Target', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 2)\n",
      "(1583691, 2)\n"
     ]
    }
   ],
   "source": [
    "#Making sure we do not have duplicates\n",
    "\n",
    "print(raw_tweets.shape)\n",
    "raw_tweets.drop_duplicates(inplace = True)\n",
    "print(raw_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries for cleaning text\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    }
   ],
   "source": [
    "#Creating a function that deletes extra letters when they are repeated more than 2 times\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "print(reduce_lengthening( \"finallllllly\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazziing\n",
      "[('amazing', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "\n",
    "word = \"amazzziiing\"\n",
    "word_wlf = reduce_lengthening(word) #calling function defined above\n",
    "print(word_wlf) #word lengthening isn't being able to fix it completely\n",
    "\n",
    "correct_word = suggest(word_wlf) \n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to clean text and git rid of punctuations, stopwords, and lowering the text\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "from pattern.en import suggest\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    #reducing charachters that repeat more than twice\n",
    "    reduced_tokens = [reduce_lengthening(token) for token in tokens]\n",
    "    #correcting the spelling of words\n",
    "    corrected = [suggest(token)[0][0] for token in reduced_tokens]\n",
    "    # filter stopwords and non english words out of document\n",
    "    filtered_tokens = [token for token in corrected if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    stem_tokens = [ps.stem(word) for word in filtered_tokens]\n",
    "    doc = ' '.join(stem_tokens)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat appl friend'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"I eat 3 upple's with my frend!\"\n",
    "normalize_document(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to clean the text in our dataframe.\n",
    "def dataframe_normalize(row):\n",
    "    row.Text = normalize_document(row.Text)\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using dask for faster process\n",
    "norm_tweets = dd.from_pandas(raw_tweets, npartitions=4*multiprocessing.cpu_count())\\\n",
    "    .map_partitions(lambda df: df.apply(dataframe_normalize,axis =1))\\\n",
    "    .compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>switchfoot httptwitpiccomyzl www summer got da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset updat facebook test might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                               Text\n",
       "0       0  switchfoot httptwitpiccomyzl www summer got da...\n",
       "1       0  upset updat facebook test might cri result sch...\n",
       "2       0  kenichan dive mani time ball manag save rest g...\n",
       "3       0                    whole bodi feel itchi like fire\n",
       "4       0                      nationwideclass behav mad see"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>switchfoot httptwitpiccomyzl www summer got da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset updat facebook test might cri result sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                               Text\n",
       "0       0  switchfoot httptwitpiccomyzl www summer got da...\n",
       "1       0  upset updat facebook test might cri result sch..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = norm_tweets[:2]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def Tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(min_df = 50,\n",
    "                                 norm = 'l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf= True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building tfidf vectorizer and get corpus feature vectors\n",
    "tfidf_vectorizer, tfidf_features = Tfidf_extractor(norm_tweets.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(norm_tweets.Text.str.split().values, size = 100, window = 30, min_count = 50, sample =1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute tfidf weighted averaged word vector for a document\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)]\n",
    "                   if tfidf_vocabulary.get(word)\n",
    "                   else 0 for word in words]\n",
    "    \n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalize above function for a corpus of documents\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors,\n",
    "                                   tfidf_vocabulary, model, num_features):\n",
    "    docs_tfidfs = [(doc.split(), doc_tfidf)\n",
    "                   for doc, doc_tfidf\n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a_raf\\Anaconda3\\envs\\Springboard\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# get tfidf weights and vocabulary\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=norm_tweets.Text.values,\n",
    "                            tfidf_vectors=tfidf_features, tfidf_vocabulary=vocab, model=model,num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wt_tfidf_word_vec_features, norm_tweets.Target.values, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.5005650298852069\n",
      "Test Accuracy = 0.5022092675509353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "print('Training Accuracy = {}'.format(lr.score(x_train, y_train)))\n",
    "print('Test Accuracy = {}'.format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(true_labels,predicted_labels),2))\n",
    "    print('Precision:', np.round(metrics.precision_score(true_labels, predicted_labels, average='weighted'),2))\n",
    "    print('Recall:', np.round(metrics.recall_score(true_labels, predicted_labels, average='weighted'),2))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(true_labels, predicted_labels, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_evaluate_model(classifier,\n",
    "                                 train_features, train_labels,\n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance  \n",
    "    get_metrics(true_labels=test_labels,\n",
    "                predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "rf = RandomForestClassifier(n_jobs= -1)\n",
    "svm = SGDClassifier(loss='hinge', n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "Precision: 0.7\n",
      "Recall: 0.7\n",
      "F1 Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict_evaluate_model(svm,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.71\n",
      "Recall: 0.71\n",
      "F1 Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict_evaluate_model(rf,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recogintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(document):\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    if isinstance(document, str):\n",
    "        document = document.lower()\n",
    "    elif isinstance(document, unicode):\n",
    "        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore').lower()\n",
    "    else:\n",
    "        raise ValueError('Document is not string or unicode!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_normalize2(row):\n",
    "    row.Text = parse_document(row.Text)\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tweets = dd.from_pandas(raw_tweets, npartitions=4*multiprocessing.cpu_count())\\\n",
    "    .map_partitions(lambda df: df.apply(dataframe_normalize2,axis =1))\\\n",
    "    .compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tweets.Target.replace(4,1, inplace =True)\n",
    "norm_tweets.Target.replace(0,-1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@switchfoot http://twitpic.com/2y1zl - awww, that's a bummer.\",\n",
       " 'you shoulda got david carr of third day to do it.',\n",
       " ';d']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_tweets.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "Person = {}\n",
    "GPE = {}\n",
    "ORG = {}\n",
    "for tweet, sent in zip(norm_tweets.Text.values,norm_tweets.Target.values):\n",
    "    doc = nlp(tweet[0])\n",
    "    for ent in doc.ents: \n",
    "        if ent.label_ == 'PERSON':\n",
    "            if ent.text in Person.keys():\n",
    "                Person[ent.text]+=sent\n",
    "            else:\n",
    "                Person[ent.text]=sent\n",
    "        elif ent.label_ == 'GPE':\n",
    "            if ent.text in GPE.keys():\n",
    "                GPE[ent.text]+=sent\n",
    "            else:\n",
    "                GPE[ent.text]=sent\n",
    "        elif ent.label_ == 'ORG': \n",
    "            if ent.text in ORG.keys():\n",
    "                ORG[ent.text]+=sent\n",
    "            else:\n",
    "                ORG[ent.text]=sent\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Person_df = pd.Series(Person,index=Person.keys())\n",
    "Person_df.sort_values(inplace = True)\n",
    "ORG_df = pd.Series(ORG, index = ORG.keys())\n",
    "ORG_df.sort_values(inplace = True)\n",
    "GPE_df = pd.Series(GPE, index = GPE.keys())\n",
    "GPE_df.sort_values(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kate              -625\n",
       "argh              -438\n",
       "u                 -331\n",
       "dang              -272\n",
       "farrah fawcett    -237\n",
       "jon               -192\n",
       "xbox              -152\n",
       "kinda sad         -126\n",
       "ed mcmahon        -124\n",
       "nyc               -113\n",
       "david carradine   -105\n",
       "jay leno          -102\n",
       "grrrr             -102\n",
       "xbox live          -66\n",
       "grrrrr             -52\n",
       "doh                -52\n",
       "sooo               -49\n",
       "throat             -49\n",
       "kinda              -47\n",
       "max                -46\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most negative sentiments for known names is for Farrah Fawcett, David Carradine, and Ed McMahon that must be because of their deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mm                  49\n",
       "god                 54\n",
       "adam                54\n",
       "hehe                57\n",
       "star trek           58\n",
       "andy                59\n",
       "miley               60\n",
       "david archuleta     62\n",
       "joe                 64\n",
       "kim                 72\n",
       "amy                 77\n",
       "tom                 83\n",
       "hun                 84\n",
       "matt                88\n",
       "yay                100\n",
       "haha               103\n",
       "woohoo             134\n",
       "harry potter       141\n",
       "chillin            144\n",
       "hannah montana     173\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person_df[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hannah Montana, Harry Potter, David Archuleta, Star Trek, and God are the known entities with positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ugh          -1073\n",
       "house         -310\n",
       "headache      -230\n",
       "grr           -223\n",
       "air france    -199\n",
       "jon &amp      -190\n",
       "at&amp;t      -186\n",
       "feelin        -147\n",
       "mac           -146\n",
       "aww           -136\n",
       "urgh          -130\n",
       "atm           -129\n",
       "doc           -126\n",
       "nooo          -122\n",
       "cat           -115\n",
       "msn           -114\n",
       "jon           -112\n",
       "bf            -106\n",
       "blah           -91\n",
       "apple          -88\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORG_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interestingly the most negative sentiments are regarding Air France and Apple products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@banksyart           78\n",
       "www.m2e.asia         79\n",
       "wooo                 80\n",
       "i &lt;3              87\n",
       "@mrtweet             95\n",
       "@kristenstewart9    110\n",
       "@therealjordin      124\n",
       "chillin             144\n",
       "@ashleytisdale      158\n",
       "@ddlovato           188\n",
       "@youngq             189\n",
       "hahaha              193\n",
       "lol                 202\n",
       "mmm                 220\n",
       "u                   221\n",
       "@jonathanrknight    252\n",
       "yay                 318\n",
       "ur                  340\n",
       "mtv                 504\n",
       "haha                728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORG_df[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Positive tweets are for MTV, Demi Lovato, Banksy, David Archie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iran        -378\n",
       "kinda       -359\n",
       "uk          -249\n",
       "us          -185\n",
       "dallas      -168\n",
       "canada      -148\n",
       "chicago     -119\n",
       "noooooo     -105\n",
       "cleveland    -97\n",
       "china        -92\n",
       "texas        -85\n",
       "booo         -83\n",
       "miami        -81\n",
       "india        -81\n",
       "houston      -78\n",
       "florida      -76\n",
       "toronto      -76\n",
       "boston       -73\n",
       "america      -63\n",
       "tehran       -61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPE_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p.s                28\n",
       "me&quot            29\n",
       "kris               31\n",
       "amsterdam          33\n",
       "berlin             35\n",
       "eminem             36\n",
       "eatin              38\n",
       "ï¿½                43\n",
       "yaaay              50\n",
       "hollywood          54\n",
       "@alyssa_milano     59\n",
       "yayyy              64\n",
       "hehehe             75\n",
       "philippines        75\n",
       "twitterville       76\n",
       "norway             90\n",
       "germany            92\n",
       "jonas             143\n",
       "yummy             214\n",
       "hehe              466\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPE_df[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_normalize3(row):\n",
    "    row.Text = cleanTxt(row.Text)\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                               Text\n",
       "0      -1    - Awww, that's a bummer.  You shoulda got Da...\n",
       "1      -1  is upset that he can't update his Facebook by ...\n",
       "2      -1   I dived many times for the ball. Managed to s...\n",
       "3      -1    my whole body feels itchy and like its on fire \n",
       "4      -1   no, it's not behaving at all. i'm mad. why am..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions\n",
    "    text = re.sub('#', '', text) # Removing '#' hash tag\n",
    "    text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n",
    " \n",
    "    return text\n",
    "\n",
    "\n",
    "# Clean the tweets\n",
    "df = dd.from_pandas(raw_tweets, npartitions=4*multiprocessing.cpu_count())\\\n",
    "    .map_partitions(lambda df: df.apply(dataframe_normalize3,axis =1))\\\n",
    "    .compute(scheduler='processes')\n",
    "\n",
    "# Show the cleaned tweets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Target.replace(-1,0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                               Text  Subjectivity  \\\n",
       "0       0    - Awww, that's a bummer.  You shoulda got Da...      0.633333   \n",
       "1       0  is upset that he can't update his Facebook by ...      0.000000   \n",
       "2       0   I dived many times for the ball. Managed to s...      0.500000   \n",
       "3       0    my whole body feels itchy and like its on fire       0.400000   \n",
       "4       0   no, it's not behaving at all. i'm mad. why am...      1.000000   \n",
       "\n",
       "   Polarity  \n",
       "0  0.216667  \n",
       "1  0.000000  \n",
       "2  0.500000  \n",
       "3  0.200000  \n",
       "4 -0.625000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to get the subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "# Create two new columns 'Subjectivity' & 'Polarity'\n",
    "def dataframe_normalize4(row):\n",
    "    row['Subjectivity'] = getSubjectivity(row.Text)\n",
    "    row['Polarity'] = getPolarity(row.Text)\n",
    "    return(row)\n",
    "\n",
    "df= dd.from_pandas(df, npartitions=4*multiprocessing.cpu_count())\\\n",
    "    .map_partitions(lambda df: df.apply(dataframe_normalize4,axis =1))\\\n",
    "    .compute(scheduler='processes')\n",
    "# Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['Analysis'] = df['Polarity'].apply(getAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "Precision: 0.66\n",
      "Recall: 0.61\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "get_metrics(df.Target.values, df.Analysis.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
